{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "\n",
    "# Nome das classes (substitua pelos nomes das suas classes)\n",
    "classes = ['Classe 1', 'Classe 2', 'Classe 3', 'Classe 4', 'Classe 5', 'Classe 6', 'Classe 7']\n",
    "\n",
    "# Dados de exemplo: Precision, Recall, e F1 para cada modelo e classe\n",
    "# Cada linha representa uma métrica para um modelo\n",
    "# Ajuste esses valores para os seus resultados reais\n",
    "precision_data = [\n",
    "    [0.8, 0.7, 0.6, 0.9, 0.5, 0.4, 0.7],  # Modelo 1 (ResNet-101)\n",
    "    [0.75, 0.65, 0.55, 0.85, 0.6, 0.5, 0.65],  # Modelo 2 (ResNet-50)\n",
    "    [0.7, 0.6, 0.5, 0.8, 0.55, 0.45, 0.6],  # Modelo 3 (GoogleNet)\n",
    "    [0.65, 0.55, 0.45, 0.75, 0.5, 0.4, 0.55]  # Modelo 4 (VGG-16)\n",
    "]\n",
    "\n",
    "# Criação dos ângulos para o radar\n",
    "num_vars = len(classes)\n",
    "angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "angles += angles[:1]  # Fechar o gráfico\n",
    "\n",
    "# Função para criar gráficos de radar\n",
    "def plot_radar(data, title, labels):\n",
    "    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
    "\n",
    "    # Desenhar uma linha e preencher para cada modelo\n",
    "    for idx, values in enumerate(data):\n",
    "        values += values[:1]  # Fechar o gráfico\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=labels[idx])\n",
    "        ax.fill(angles, values, alpha=0.1)\n",
    "\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(classes, fontsize=10)\n",
    "    plt.title(title, size=15, color='black', y=1.1)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.1, 1.1))\n",
    "    plt.show()\n",
    "\n",
    "# Exibir gráficos de radar para precisão, recall e F1\n",
    "plot_radar(precision_data, 'Precision - Modelos', ['ResNet-101', 'ResNet-50', 'GoogleNet', 'VGG-16'])\n",
    "# Repita para os dados de recall e F1 se tiver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR, SequentialLR\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision.models.convnext import (convnext_tiny, ConvNeXt_Tiny_Weights,\n",
    "                                         convnext_small, ConvNeXt_Small_Weights,\n",
    "                                         convnext_base, ConvNeXt_Base_Weights,\n",
    "                                         convnext_large, ConvNeXt_Large_Weights)\n",
    "from torchvision.models.swin_transformer import (swin_t, Swin_T_Weights,\n",
    "                                                 swin_s, Swin_S_Weights,\n",
    "                                                 swin_b, Swin_B_Weights)\n",
    "import torchmetrics\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import config\n",
    "from compact_transform.src import cct_14_7x2_224, cct_14_7x2_384, cct_14_7x2_384_fl\n",
    "\n",
    "\n",
    "\n",
    "class CustomModel(pl.LightningModule):\n",
    "    def __init__(self, tmodel, epochs, learning_rate, scale_factor,\n",
    "                 drop_path_rate, num_classes, label_smoothing, optimizer_momentum):\n",
    "        \n",
    "        super(CustomModel, self).__init__()\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.scale_factor = scale_factor\n",
    "        self.drop_path_rate = drop_path_rate\n",
    "        self.num_classes = num_classes\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.optimizer_momentum = optimizer_momentum\n",
    "        self.fn_loss = nn.CrossEntropyLoss(label_smoothing=self.label_smoothing)\n",
    "        \n",
    "        # Métricas\n",
    "        self.train_accuracy = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.val_accuracy = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.test_accuracy = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "\n",
    "        self.model = models.convnext_tiny(weights=ConvNeXt_Tiny_Weights.DEFAULT, \n",
    "                                        drop_path_rate=self.drop_path_rate)\n",
    "        self.sequential_layers = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.LayerNorm(768, eps=1e-6, elementwise_affine=True),\n",
    "            nn.Linear(in_features=768, out_features=self.num_classes, bias=True)\n",
    "        )\n",
    "        self.model.classifier = self.sequential_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        logits = self.forward(images)\n",
    "        loss = self.fn_loss(logits, labels)\n",
    "        preds = torch.argmax(logits, 1)\n",
    "\n",
    "        # Calcular a precisão\n",
    "        self.train_accuracy(preds, labels)\n",
    "        \n",
    "        # Logar a perda e a acurácia\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('train_accuracy', self.train_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        \n",
    "        # Retornar a perda para o processamento posterior\n",
    "        return {'loss': loss}\n",
    "    \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        logits = self.forward(images)\n",
    "        loss = self.fn_loss(logits, labels)\n",
    "        preds = torch.argmax(logits, 1)\n",
    "        \n",
    "        # Calcular a precisão para validação\n",
    "        self.val_accuracy(preds, labels)\n",
    "        \n",
    "        # Logar a perda e a acurácia no conjunto de validação\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('val_accuracy', self.val_accuracy, prog_bar=True, on_epoch=True)\n",
    "        \n",
    "        # Retornar a perda e a acurácia\n",
    "        return {'val_loss': loss}\n",
    "    \n",
    "    # def on_train_epoch_end(self):\n",
    "    #     # Acessar a perda média do treino automaticamente através do logger\n",
    "    #     avg_loss = self.trainer.callback_metrics['train_loss']\n",
    "\n",
    "    #     # Imprimir a perda média de treino\n",
    "    #     print(f'Loss médio do treino na época: {avg_loss:.4f}')\n",
    "    \n",
    "    # def on_validation_epoch_end(self):\n",
    "    #     # Acessar a perda média da validação automaticamente através do logger\n",
    "    #     avg_val_loss = self.trainer.callback_metrics['val_loss']\n",
    "\n",
    "    #     # Imprimir a perda média da validação\n",
    "    #     print(f'Loss médio da validação na época: {avg_val_loss:.4f}')\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        logits = self.forward(images)\n",
    "        loss = self.fn_loss(logits, labels)\n",
    "        preds = torch.argmax(logits, 1)\n",
    "\n",
    "        # Calcular a precisão\n",
    "        self.test_accuracy(preds, labels)   \n",
    "        self.log(\"test/loss_epoch\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"test/acc_epoch\", self.test_accuracy, on_step=False, on_epoch=True)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Definir o otimizador com os grupos de parâmetros\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, betas = self.optimizer_momentum)\n",
    "\n",
    "        # Definir o scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.epochs)\n",
    "\n",
    "        # Retornar o otimizador e o scheduler\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'epoch',  # Step the scheduler per epoch\n",
    "                'monitor': 'val_loss',  # Optional, monitor val_loss (useful for other schedulers)\n",
    "                'frequency': 1,  # Apply the scheduler every epoch\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        logits = self.forward(images)\n",
    "        loss = self.fn_loss(logits, labels)\n",
    "        preds = torch.argmax(logits, 1)\n",
    "\n",
    "        # Calcular a precisão\n",
    "        self.train_accuracy(preds, labels)\n",
    "        \n",
    "        # Logar a perda e a acurácia\n",
    "        self.log('train_loss', loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log('train_accuracy', self.train_accuracy, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        \n",
    "        # Retornar a perda para o processamento posterior\n",
    "        return {'loss': loss}\n",
    "    \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        logits = self.forward(images)\n",
    "        loss = self.fn_loss(logits, labels)\n",
    "        preds = torch.argmax(logits, 1)\n",
    "        \n",
    "        # Calcular a precisão para validação\n",
    "        self.val_accuracy(preds, labels)\n",
    "        \n",
    "        # Logar a perda e a acurácia no conjunto de validação\n",
    "        self.log('val_loss', loss, prog_bar=True, on_epoch=True)\n",
    "        self.log('val_accuracy', self.val_accuracy, prog_bar=True, on_epoch=True)\n",
    "        \n",
    "        # Retornar a perda e a acurácia\n",
    "        return {'val_loss': loss}\n",
    "    \n",
    "    # def on_train_epoch_end(self):\n",
    "    #     # Acessar a perda média do treino automaticamente através do logger\n",
    "    #     avg_loss = self.trainer.callback_metrics['train_loss']\n",
    "\n",
    "    #     # Imprimir a perda média de treino\n",
    "    #     print(f'Loss médio do treino na época: {avg_loss:.4f}')\n",
    "    \n",
    "    # def on_validation_epoch_end(self):\n",
    "    #     # Acessar a perda média da validação automaticamente através do logger\n",
    "    #     avg_val_loss = self.trainer.callback_metrics['val_loss']\n",
    "\n",
    "    #     # Imprimir a perda média da validação\n",
    "    #     print(f'Loss médio da validação na época: {avg_val_loss:.4f}')\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        logits = self.forward(images)\n",
    "        loss = self.fn_loss(logits, labels)\n",
    "        preds = torch.argmax(logits, 1)\n",
    "\n",
    "        # Calcular a precisão\n",
    "        self.test_accuracy(preds, labels)   \n",
    "        self.log(\"test/loss_epoch\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"test/acc_epoch\", self.test_accuracy, on_step=False, on_epoch=True)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Definir o otimizador com os grupos de parâmetros\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, betas = self.optimizer_momentum)\n",
    "\n",
    "        # Definir o scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.epochs)\n",
    "\n",
    "        # Retornar o otimizador e o scheduler\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'epoch',  # Step the scheduler per epoch\n",
    "                'monitor': 'val_loss',  # Optional, monitor val_loss (useful for other schedulers)\n",
    "                'frequency': 1,  # Apply the scheduler every epoch\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "def load_hyperparameters(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        hyperparams = yaml.safe_load(file)  # Carregar o YAML\n",
    "    return hyperparams\n",
    "\n",
    "hyperparams = load_hyperparameters('config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/swin_v2_t-b137f0e2.pth\" to /home/rodrigoduarte/.cache/torch/hub/checkpoints/swin_v2_t-b137f0e2.pth\n",
      "100%|██████████| 109M/109M [00:11<00:00, 10.3MB/s] \n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torchvision.models import swin_v2_t, Swin_V2_T_Weights\n",
    "model = models.swin_v2_t(weights=Swin_V2_T_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (features): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): Permute()\n",
       "      (2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): SwinTransformerBlockV2(\n",
       "        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttentionV2(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=3, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlockV2(\n",
       "        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttentionV2(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=3, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.018181818181818184, mode=row)\n",
       "        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): PatchMergingV2(\n",
       "      (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): SwinTransformerBlockV2(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttentionV2(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=6, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.03636363636363637, mode=row)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlockV2(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttentionV2(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=6, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05454545454545456, mode=row)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): PatchMergingV2(\n",
       "      (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): SwinTransformerBlockV2(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttentionV2(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07272727272727274, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlockV2(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttentionV2(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09090909090909091, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): SwinTransformerBlockV2(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttentionV2(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.10909090909090911, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): SwinTransformerBlockV2(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttentionV2(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1272727272727273, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): SwinTransformerBlockV2(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttentionV2(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.14545454545454548, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): SwinTransformerBlockV2(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttentionV2(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.16363636363636364, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): PatchMergingV2(\n",
       "      (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): SwinTransformerBlockV2(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttentionV2(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.18181818181818182, mode=row)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlockV2(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttentionV2(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (cpb_mlp): Sequential(\n",
       "            (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (permute): Permute()\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de classes: 32\n",
      "Classes: ['Anhui Barberry', 'Chinese cinnamon', 'trident maple', 'Chinese redbud', 'oleander', 'Chinese horse chestnut', 'Chinese Toon', 'wintersweet', 'deodar', 'Japan Arrowwood', 'Glossy Privet', 'camphortree', 'Japanese cheesewood', 'pubescent bamboo', 'peach', 'true indigo', 'sweet osmanthus', 'Chinese tulip tree', 'Japanese Flowering Cherry', 'yew plum pine', 'goldenrain tree', 'Big-fruited Holly', \"Beale's barberry\", 'Japanese maple', 'ginkgo, maidenhair tree', 'tangerine', 'castor aralia', 'Canadian poplar', 'Crape myrtle, Crepe myrtle', 'southern magnolia', 'Nanmu', 'Ford Woodlotus']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(32,\n",
       " ['Anhui Barberry',\n",
       "  'Chinese cinnamon',\n",
       "  'trident maple',\n",
       "  'Chinese redbud',\n",
       "  'oleander',\n",
       "  'Chinese horse chestnut',\n",
       "  'Chinese Toon',\n",
       "  'wintersweet',\n",
       "  'deodar',\n",
       "  'Japan Arrowwood',\n",
       "  'Glossy Privet',\n",
       "  'camphortree',\n",
       "  'Japanese cheesewood',\n",
       "  'pubescent bamboo',\n",
       "  'peach',\n",
       "  'true indigo',\n",
       "  'sweet osmanthus',\n",
       "  'Chinese tulip tree',\n",
       "  'Japanese Flowering Cherry',\n",
       "  'yew plum pine',\n",
       "  'goldenrain tree',\n",
       "  'Big-fruited Holly',\n",
       "  \"Beale's barberry\",\n",
       "  'Japanese maple',\n",
       "  'ginkgo, maidenhair tree',\n",
       "  'tangerine',\n",
       "  'castor aralia',\n",
       "  'Canadian poplar',\n",
       "  'Crape myrtle, Crepe myrtle',\n",
       "  'southern magnolia',\n",
       "  'Nanmu',\n",
       "  'Ford Woodlotus'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_classes(dataset_dir):\n",
    "    # Lista de subdiretórios dentro do diretório do dataset\n",
    "    classes = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
    "    num_classes = len(classes)\n",
    "    \n",
    "    print(f\"Número de classes: {num_classes}\")\n",
    "    print(\"Classes:\", classes)\n",
    "    \n",
    "    return num_classes, classes\n",
    "\n",
    "# Exemplo de uso\n",
    "train_dir = \"/home/rodrigoduarte/Documentos/projeto/imagens/flavia/train\"  # Substitua pelo caminho para o diretório de treino\n",
    "count_classes(train_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mestrado",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
